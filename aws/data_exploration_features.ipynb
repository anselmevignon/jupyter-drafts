{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## loading data/libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-20T19:09:44.933314",
     "start_time": "2017-01-20T19:09:44.913874"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "\n",
    "from bokeh.charts import output_notebook, Scatter, Bar, show, output_file, Line, BoxPlot, Scatter\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.layouts import row, column, gridplot\n",
    "\n",
    "from ipywidgets import interactive\n",
    "from IPython.display import display\n",
    "from IPython.utils.py3compat import annotate\n",
    "\n",
    "output_notebook() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-20T18:12:25.340898",
     "start_time": "2017-01-20T18:12:25.168430"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "INPUT=\"data/device_failure.csv\" \n",
    "dataset = pd.read_csv(INPUT,index_col=[0,1],parse_dates=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## features\n",
    " \n",
    " Per features:\n",
    " \n",
    " ### Statistical distribution:\n",
    " - Distribution\n",
    " - Distribution over failures\n",
    " - Distribution over devices\n",
    " - Distribution over failing devices\n",
    " \n",
    "###  Temporal distribution\n",
    " - Average value over time\n",
    " - Average value over time for failing devices\n",
    " - Value before failure   \n",
    " \n",
    "###  Frequency distribution\n",
    " - DFT / device\n",
    " - DFT / failing device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@annotate(attribute=list(dataset.columns[1:]))\n",
    "def pick_attribute(attribute):\n",
    "    return \"current attribute=%s\" % s\n",
    "s = interactive(pick_attribute)\n",
    "display(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## building data objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "attribute = s.children[0].value\n",
    "feature_dset = dataset[[attribute,\"failure\"]]\n",
    "failing_points = feature_dset[feature_dset[\"failure\"]>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def failure_date(failure):\n",
    "    data = feature_dset.ix[failure.index]\n",
    "    dates =data[data[\"failure\"]>0]\n",
    "    if not dates.empty:\n",
    "        return dates.iloc[0].name[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "devices = feature_dset.groupby(level=1).agg(\n",
    "    {\n",
    "        \"failure\":{\n",
    "            \"failure\":np.sum, \n",
    "            \"failure_date\":failure_date},\n",
    "        attribute : {\n",
    "            \"min_att\":np.min,\n",
    "            \"max_att\":np.max,\n",
    "            \"mean_att\":np.mean,\n",
    "            \"std_att\":np.std\n",
    "        }})\n",
    "devices.columns = devices.columns.droplevel()\n",
    "\n",
    "failing_devices = devices[devices[\"failure\"]>0]\n",
    "working_devices = devices[devices[\"failure\"]==0]\n",
    "\n",
    "\n",
    "working_devices_t = pd.DataFrame({attribute:feature_dset[attribute].unstack().filter(items=working_devices.index).unstack()}).dropna()\n",
    "failing_devices_t = pd.DataFrame({attribute:feature_dset[attribute].unstack().filter(items=failing_devices.index).unstack()}).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-20T19:22:03.055475",
     "start_time": "2017-01-20T19:22:02.517595"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_hist(df, column,label = None,w = 450,h=300,bins=20,color=\"lightblue\"):\n",
    "    mu = df[column].mean()\n",
    "    std = df[column].std()\n",
    "    label = label if label else column\n",
    "    title = u\"%s (μ=%.2e, σ=%.2e)\" % (label, mu,std)\n",
    "    f = figure(title=title,width=w,height=h,y_axis_type=\"log\") \n",
    "    hist, edges = np.histogram(df[column], density=True, bins=bins)\n",
    "    f.quad(\n",
    "        top=hist, \n",
    "        bottom=0,\n",
    "        left=edges[:-1],\n",
    "        right=edges[1:],\n",
    "        fill_color=color,\n",
    "        line_color=\"grey\")\n",
    "    return f\n",
    "\n",
    "p0 = build_hist(failing_points,attribute,u\"%s for failing points\" % attribute,color=\"green\")\n",
    "p1 = build_hist(feature_dset,attribute)\n",
    "\n",
    "h = row(p0,p1)\n",
    "show(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def per_car(car):\n",
    "    p0 = build_hist(failing_devices,car,label=\"%s on failing devices\"% car)\n",
    "    p1 = build_hist(working_devices,car,label=\"%s on working devices\"% car)\n",
    "    return [p0,p1]\n",
    "\n",
    "\n",
    "show(gridplot([ per_car(c) for c in set(c for c in devices.columns if \"att\" in c ) ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~attrbibute 5 : some failing points might be controlled by too much variation in this attribute~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Value over Time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dev_std(device_df):\n",
    "    return device_df.rolling(window=20,center=False).std()\n",
    "\n",
    "def roll_std(df):\n",
    "    return df[attribute].groupby(level=\"device\").transform(dev_std)\n",
    "\n",
    "    \n",
    "failing_devices_t[\"rolling_std\"]  = roll_std(failing_devices_t)\n",
    "working_devices_t[\"rolling_std\"]  = roll_std(working_devices_t)\n",
    "\n",
    "time = pd.DataFrame({\n",
    "    attribute : feature_dset[attribute].groupby(level=0).mean(),\n",
    "    \"%s for failing devices\" %attribute : failing_devices_t[attribute].groupby(level=1).mean(),\n",
    "    \"%s for working devices\" %attribute : working_devices_t[attribute].groupby(level=1).mean()\n",
    "    #\"rolling std(%s) for failing devices\" % attribute : failing_devices_t[\"rolling_std\"].groupby(level=1).mean(),\n",
    "    #\"rolling std(%s) for working devices\" % attribute : working_devices_t[\"rolling_std\"].groupby(level=1).mean()\n",
    "})\n",
    "show(Line(time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attribute1 : \n",
    "\n",
    "- ~~Hyp 1 : amplitude gets higher when the device start failing => disproved~~\n",
    "- ~~Hyp 2 : amplitude is always higher for more fragile devices => disproved~~\n",
    "- ~~Hyp 2 : wider amplitude for SOME signals~~\n",
    "- ~~Hyp 3 : failing devices somehow synchronize, resonnance effect (unlikely. plus what would it mean ?)~~\n",
    "- Hyp 4 : too few devices to average out see graph \"n_devices\" [here][1]\n",
    "[1]: data_exploration.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# relative time building\n",
    "def to_relative_time(df,device_to_endtime,rel_time_threshold=-100):\n",
    "    temp = df.reset_index()\n",
    "    temp[\"failure_date\"]=temp[\"device\"].map(device_to_endtime)\n",
    "    temp[\"dt_from_fail\"]= (temp[\"date\"]-temp[\"failure_date\"] )/ np.timedelta64(1,'D')\n",
    "    relative_time = temp.set_index([\"device\",\"dt_from_fail\"])\n",
    "    \n",
    "    # filter relative values\n",
    "    relative_time = relative_time[relative_time.index.get_level_values(\"dt_from_fail\") >= rel_time_threshold]\n",
    "    relative_time = relative_time[relative_time.index.get_level_values(\"dt_from_fail\") <0 ]\n",
    "\n",
    "    return relative_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rel_time_threshold = -100\n",
    "n_samples = 50\n",
    "\n",
    "# relative time is relative to the failure date for negatives\n",
    "fail_end_dates= devices[\"failure_date\"].dropna().to_dict()\n",
    "fail_relative_time = to_relative_time(failing_devices_t,fail_end_dates,rel_time_threshold)\n",
    "\n",
    "# for working ones, we use the last value (beware, could lead to weird effects, if the attribute changes over time)\n",
    "work_end_dates= working_devices_t.reset_index(level=\"date\")[\"date\"].groupby(level=0).max().to_dict()\n",
    "work_relative_time = to_relative_time(working_devices_t,work_end_dates,rel_time_threshold)\n",
    "\n",
    "fail_rel_sampled = fail_relative_time[attribute].unstack(level=\"dt_from_fail\").sample(n=n_samples).stack()\n",
    "work_rel_sampled = work_relative_time[attribute].unstack(level=\"dt_from_fail\").sample(n=n_samples).stack()\n",
    "\n",
    "show(row(\n",
    "    Line(\n",
    "        fail_rel_sampled.unstack(level=\"device\"),\n",
    "        width=450,\n",
    "        height=400,\n",
    "        title =\"%s before failure for a sample failing devices\" % attribute,\n",
    "        legend=None),\n",
    "    Line(\n",
    "        work_rel_sampled.unstack(level=\"device\"),\n",
    "        width=450,\n",
    "        height=400,\n",
    "        title =\"%s before end for a sample working devices\" % attribute,\n",
    "        legend=None)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attribute 4: Need to take into account the Dv aver time, in addition to the dt\n",
    "\n",
    "attribute 6 : we can see two classes of population: the ones with increasing attr6, and the ones without"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples=20\n",
    "sampled_working_devices = working_devices_t[attribute].unstack(level=\"date\").sample(n=n_samples).stack()\n",
    "sampled_failing_devices = failing_devices_t[attribute].unstack(level=\"date\").sample(n=n_samples).stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l0 = Line(\n",
    "    sampled_failing_devices.unstack(level=\"device\"),\n",
    "    width=450,\n",
    "    height=400,\n",
    "    title='%s for sampled failing devices' % attribute,\n",
    "    legend=None\n",
    ")\n",
    "l1 = Line(\n",
    "    sampled_working_devices.unstack(level=\"device\"),\n",
    "    width=450,\n",
    "    height=400,\n",
    "    title = '%s for sampled working devices' % attribute,\n",
    "    legend=None\n",
    ")\n",
    "\n",
    "show(row(l0,l1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft\n",
    "fft_df = feature_dset[[attribute]].copy()\n",
    "\n",
    "def to_fft(df):\n",
    "    resampled =  df.resample(\"1D\",level=\"date\").mean().fillna(method='pad')\n",
    "    n = len(resampled)\n",
    "    return np.abs(fft(resampled))[n//2:]\n",
    "fft_per_device = fft_df[attribute].groupby(level=\"device\",sort=True).transform(to_fft)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fft_df[\"df\"] = fft_per_device\n",
    "fft_plot = fft_df.groupby(level=\"device\").apply(lambda x: x.reset_index(drop=True))[\"df\"]\n",
    "\n",
    "fft_working_devices = fft_plot.unstack(level=0).filter(items=working_devices.index).stack()\n",
    "fft_failing_devices = fft_plot.unstack(level=0).filter(items=failing_devices.index).stack()\n",
    "\n",
    "n_samples = 100\n",
    "to_plot_working = fft_working_devices.unstack(level=1).sample(n=10).unstack().dropna()\n",
    "to_plot_failing = fft_failing_devices.unstack(level=0).sample(n=10).unstack().dropna()\n",
    "show(row(\n",
    "    Line(to_plot_failing.unstack(\"device\"),\n",
    "        width=450,\n",
    "        height=400,\n",
    "        title = \"dft of %s for sampled failing devices\" % attribute,\n",
    "        legend=None),\n",
    "    Line(to_plot_working.unstack(\"device\"),\n",
    "        width=450,\n",
    "        height=400,\n",
    "        title = \"dft of %s for sampled working device\" % attribute,\n",
    "        legend=None),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attribute 6 : kampai !!\n",
    "attribute 9 : good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## specific analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if attribute == u\"attribute3\":\n",
    "    dd = feature_dset\n",
    "    dd[dd[attribute]>0]\n",
    "    dd = dd.swaplevel().sort_index()\n",
    "    grouped= dd[dd[attribute]>0].groupby(level=\"device\").agg(lambda x : len(np.unique(x)))[attribute]\n",
    "    weird_devices = set(grouped[grouped>1].index)\n",
    "    print weird_devices\n",
    "    weird_values = dd.unstack(level=\"date\").filter(items=weird_devices,axis=\"index\").stack()\n",
    "    print weird_values[\"failure\"].value_counts()\n",
    "    print weird_values.groupby(level=\"device\").apply(lambda df: df[attribute].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attribute 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# attribute 5: testing if variations are not wider for some failures..\n",
    "print (failing_devices[\"max_att\"] -failing_devices[\"min_att\"]).value_counts()\n",
    "print (working_devices[\"max_att\"] -working_devices[\"min_att\"]).value_counts()\n",
    "\n",
    "# answer: Nope. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {
    "ebcbb30ae1de41ea8ce60d17e3d22d38": {
     "views": [
      {
       "cell_index": 4
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
