{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. device model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook: [here](model_per_device.ipynb)\n",
    "\n",
    "## Getting started\n",
    "\n",
    "First model :\n",
    "    - XDGBoost : overfit \n",
    "        result : accuracy = 0.93\n",
    "            \n",
    "\n",
    "==> trying RandomForest instead\n",
    "       better, less overfit\n",
    "       \n",
    "- increasing the nb folds during hyperpparameters\n",
    "- removing the features that should not make sense:\n",
    "    - attribute1\n",
    "    - min_any and std_any\n",
    "    \n",
    "==> 'reasonable' model, 93% accuracy, but much less overfit\n",
    "\n",
    " - removing suspicious positives (see dataset exploration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going through features...\n",
    "\n",
    " - attribute1 : removed\n",
    " - attribute2 :\n",
    "   - useful as raw feature (max & mean)\n",
    "   - removing it lower perfs : accuracy : 92.4 --> 92.4... so no use\n",
    "   - adding dt_attribute2 ==> 92.4-> 92.77\n",
    "   - addinf dt2_attribute2 ==> 92.7 -> 93.2 (93.5 with calibration)\n",
    " - attribute3: removed\n",
    " - attribute4 : with DFT : 93.2=> 93.8\n",
    " - attribute5 : with DFT : 94.06\n",
    " - attribute6, 7 : DFT does not bring much : corelation already extracted ? (the feature seems important )\n",
    " - attribute9: no effet... why ?\n",
    " \n",
    " feature filtering: no discernable effect\n",
    " feature scaling, PCA => idem\n",
    " \n",
    " ... all of this is reasonable, considering we are trying tree models\n",
    " \n",
    " testing SVM :  gives goood results, still under random forest (0.91 rather than 0.93)\n",
    " \n",
    " re-testing Gradient boosted trees : still slightly under Randomforest\n",
    " \n",
    "## TPOT\n",
    " \n",
    "96.4% accuracy ! (@ 95%, random forest with tuned voting)\n",
    " \n",
    " \n",
    "Caveat: there seems to be problem of overfitting on most models. The best recommentdation IRL would be to get more data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Temporal Model\n",
    "\n",
    "Notebook: [here](model_per_device_and_time.ipynb)\n",
    "\n",
    "## Set up first model:\n",
    "\n",
    "Windowing groupby:  need to adapt the aggregation methods to be commpatible with rolling aggregation\n",
    "     \n",
    "Per device test/train split strategy:\n",
    "    \n",
    "Because the data we aggregate for each time point is very correlated with other timepoint for the same device, we need to make sure there are no selection bias, when building our model.\n",
    "    \n",
    "This is done by splitting the examples per device: a device with examples in the test set cannot also have examples in the train set. This is basicaly assured by using specific splitting startegy provided by sklearn\n",
    "    \n",
    "Each time point with no failure is a negative, that leads to very unbalanced classes:\n",
    "    - We use 'f1' scoring method rather than acccuracy, to be less sensible to class imbalance \n",
    "    - We subsample negatives, to reduce this imbalance. \n",
    "    - To be tested: oversample positives rather than subsample negatives\n",
    "    - We extend the positive window by 7 days: every device detected to be failing at most 7 days before a failure, is considered a negative \n",
    "\n",
    " \n",
    "## Feature engineering\n",
    "     \n",
    "After calibration, we added to the features an averaging lookback window, to use as feature the \"last\" value observed for an attribute, in addition to its averaged value.\n",
    "     \n",
    "Here, trimmming the useless features seems to have a positive impact on the performances.\n",
    " \n",
    "## Test other models\n",
    " \n",
    "The same models are tested: XGboost, Random Forest, and SVM.\n",
    "    - best RF : f1=0.47 (but unstable model, some RF perform better than others)\n",
    "\n",
    "We also test PCA, feature scaling, and feature filtering\n",
    "\n",
    "# 3. Model bagging\n",
    "\n",
    "This done at validation time in [this notebook](Model Validation.ipynb)\n",
    "\n",
    "Tried two bagging methods: \n",
    " - consensus: averaged score is taken as final score\n",
    " - vote:  the most confident model perform the prediction\n",
    " \n",
    "Further development: try to  use two models in a boosting scheme, rather than a bagging one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
