{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-24T19:51:26.963903",
     "start_time": "2017-01-24T19:51:26.658611"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "\n",
    "from bokeh.charts import output_notebook, Scatter, Bar, show, output_file, Line, BoxPlot, Scatter\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.layouts import row, column, gridplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-24T19:51:27.187239",
     "start_time": "2017-01-24T19:51:27.176070"
    },
    "collapsed": false,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "from bokeh.charts import output_notebook, Scatter, Bar, show, output_file, Line, BoxPlot, Scatter\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.layouts import column, row\n",
    "from bokeh.io import hplot\n",
    "output_notebook() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-24T19:51:28.429061",
     "start_time": "2017-01-24T19:51:28.259257"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "INPUT=\"data/device_failure.csv\" \n",
    "dataset = pd.read_csv(INPUT,index_col=[0,1],parse_dates=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## per device model\n",
    "\n",
    " - Set up first model\n",
    " - Precision/recall, ROC\n",
    " - Calibration\n",
    " - PCA ?\n",
    " - feature engineering\n",
    " - data cleaning\n",
    " - Test other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-24T19:51:29.928777",
     "start_time": "2017-01-24T19:51:29.925844"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fitering methods\n",
    "suspicious_positives = set([\"S1F0GPFZ\", \"S1F136J0\", \"W1F0KCP2\", \"W1F0M35B\", \"W1F11ZG9\"])\n",
    "def filter_devices(df):\n",
    "    return df.filter(axis=\"index\",items=(set(df.index) - suspicious_positives)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-24T19:51:30.579069",
     "start_time": "2017-01-24T19:51:30.572802"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature preprocessing\n",
    "def build_deriv(df,c,n=1):\n",
    "    def per_device(per_device):\n",
    "        clean_index = per_device.reset_index(level=1,drop=True)\n",
    "        resampled=clean_index.resample('1D').pad()\n",
    "        \n",
    "        raw_diff = np.diff(resampled,n=n)\n",
    "        #fill the series start with zeros\n",
    "        while (len(raw_diff) < len(resampled)):\n",
    "            raw_diff = np.insert(raw_diff,0,0)\n",
    "        d = pd.Series(data=raw_diff,index = resampled.index )\n",
    "        #print d\n",
    "        return d.dropna()[d>0]\n",
    "    some_d= df[c].groupby(level=\"device\").apply(per_device)\n",
    "    return some_d.swaplevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-24T19:51:31.361300",
     "start_time": "2017-01-24T19:51:31.344405"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft\n",
    "#fft_df = feature_dset[[attribute]].copy()\n",
    "\n",
    "def to_fft(df):\n",
    "    try:\n",
    "        resampled =  df.resample(\"1D\",level=\"date\").mean().fillna(method='pad')\n",
    "    except:\n",
    "        #if we cannot resample, this (usually) means that we are using a rolling aggregation, outputing\n",
    "        #an nd.array rather than a df. the good news is, in this case I shoudl already have resampled.\n",
    "        resampled = df.copy()\n",
    "    n = len(resampled)\n",
    "    return np.abs(fft(resampled))[n//2:]\n",
    "\n",
    "def fft_line(df):\n",
    "    print df\n",
    "    return df.groupby(level=\"device\",sort=True).transform(to_fft)\n",
    "\n",
    "def peaks(line):\n",
    "    sorted_by_used = sorted(enumerate(line),key = lambda  t: t[1], reverse=True )\n",
    "    boundaries = set()\n",
    "    peaks = []\n",
    "    for i,value in sorted_by_used:\n",
    "        if i not in boundaries:\n",
    "            peaks.append((i,value))\n",
    "        # in any case, i neighbors cannot be peaks now.\n",
    "        boundaries.add(i+1)\n",
    "        boundaries.add(i-1)\n",
    "    return peaks\n",
    "            \n",
    "    \n",
    "def fft_peak(df,p=0,index_no_value=True):\n",
    "    fft = to_fft(df)\n",
    "    #fft = fft_line(df)\n",
    "    all_peaks = peaks(fft.tolist())\n",
    "    if (len(all_peaks) > p):\n",
    "        return all_peaks[p][0 if index_no_value else 1]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-24T19:51:32.192583",
     "start_time": "2017-01-24T19:51:32.185260"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pre_filter(df):\n",
    "    res = df.copy()\n",
    "    del res[\"attribute1\"]\n",
    "    del res[\"attribute3\"]\n",
    "    #del res[\"attribute5\"]\n",
    "    dt_list = [\"attribute2\"]#,\"attribute8\"]\n",
    "    for c in dt_list:\n",
    "        deriv = build_deriv(res,c)\n",
    "        res[\"dt_%s\" % c] = deriv\n",
    "        res[\"dt2_%s\" % c] =  build_deriv(res,c,2)\n",
    "    return res.fillna(0)\n",
    "\n",
    "def post_filter(df):\n",
    "    res = df.copy()\n",
    "    res = filter_devices(res)\n",
    "    for col in res.columns:\n",
    "        if \"min\" in col:\n",
    "            del res[col]\n",
    "        if \"std\" in col:\n",
    "            del res[col]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-24T19:51:38.538772",
     "start_time": "2017-01-24T19:51:32.968796"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pre_dataset = pre_filter(dataset)\n",
    "#print feature_set.columns\n",
    "\n",
    "features = [f for f in pre_dataset.columns if \"att\" in f]\n",
    "def f_to_dict(feature):\n",
    "    d = {\n",
    "            \"min_%s\" % feature:np.min,\n",
    "            \"max_%s\" % feature:np.max,\n",
    "            \"mean_%s\" % feature :np.mean,\n",
    "            \"std_%s\" % feature:np.std\n",
    "        }\n",
    "    dft_list = [\"attribute4\",\"attribute5\", \"attribute6\",\"attribute7\",\"attribute9\"]\n",
    "    if feature in dft_list:\n",
    "        d[\"dft_p0_ind%s\" % feature] = lambda r : fft_peak(r,p=0,index_no_value=True)\n",
    "        d[\"dft_p0_val%s\" % feature] = lambda r : fft_peak(r,p=0,index_no_value=False)\n",
    "        #d[\"dft_p1_ind%s\" % feature] = lambda r : fft_peak(r,p=1,index_no_value=True)\n",
    "        #d[\"dft_p1_val%s\" % feature] = lambda r : fft_peak(r,p=1,index_no_value=False)\n",
    "    return d\n",
    "\n",
    "agg_dict = dict( (f,f_to_dict(f)) for f in features )\n",
    "#print agg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-24T19:51:38.554781",
     "start_time": "2017-01-24T19:51:38.540588"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bugfix: rolling aggregation after group by does not handle multiple aggregation per column..\n",
    "# we fix this by flattening the aggregation dict and repeating the data within the dataframe\n",
    "final_columns = [k for c in agg_dict for k in agg_dict[c]]\n",
    "input_columns = [c for c in agg_dict for k in agg_dict[c]]\n",
    "flat_agg_dict = dict( (k,agg_dict[c][k]) for c in agg_dict for k in agg_dict[c])\n",
    "dup_dataset = pre_dataset[input_columns]\n",
    "dup_dataset.columns = final_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-24T19:51:38.569264",
     "start_time": "2017-01-24T19:51:38.557847"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resample_per_device(df):\n",
    "    if dataset.index.names == [\"device\",\"date\"]:\n",
    "        df = df.swaplevel().sort_index()\n",
    "    groups = df.groupby(level=\"device\")\n",
    "    sampled = (\n",
    "        groups.get_group(g).reset_index(level=\"device\").resample(\"1D\").pad().reset_index()\n",
    "        for g in groups.groups)\n",
    "    return pd.concat(sampled).set_index([\"date\",\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-24T19:52:55.830896",
     "start_time": "2017-01-24T19:51:38.571722"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# This is where magic happens  !!!\n",
    "# instead of simply grouping by, we roll over the dataset...\n",
    "# using the \"hacky\" flat version, to avoid issues.\n",
    "# The hack increase the memory consumption quite a lot, but it should still be better than\n",
    "# explicitely building the windowed lines before aggregating over it. \n",
    "#\n",
    "\n",
    "feature_set = resample_per_device(dup_dataset) \\\n",
    "    .groupby(level=\"device\",as_index=False) \\\n",
    "    .rolling(window=360,min_periods=1) \\\n",
    "    .agg(flat_agg_dict) \\\n",
    "    .reset_index(level=0,drop=True) \n",
    "feature_set = post_filter(feature_set)\n",
    "\n",
    "# feature filtering\n",
    "# feature filtering\n",
    "#try : \n",
    "#    filtered = feature_set.filter(items=feature_imp.index)\n",
    "#    print \"filtering devices\"\n",
    "#    feature_set = filtered\n",
    "#except:\n",
    "#    print \"no feature filtering\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-24T19:53:00.341839",
     "start_time": "2017-01-24T19:52:55.832136"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# use label_window to expand label to neighboring days.\n",
    "# basically, a mainrtenance x days before failure is still OK\n",
    "#\n",
    "label_window = 2\n",
    "\n",
    "label_set =  resample_per_device(dataset[[\"failure\"]]) \\\n",
    "    .sortlevel(level=\"date\",ascending=False) \\\n",
    "    .groupby(level=\"device\",as_index=False) \\\n",
    "    .rolling(window=label_window, min_periods=1) \\\n",
    "    .sum() \\\n",
    "    .reset_index(level=0,drop=True) \n",
    "label_set = filter_devices(label_set)\n",
    "feature_mat = feature_set.to_sparse().as_matrix()\n",
    "label_mat = label_set.as_matrix().ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-24T19:53:05.836898",
     "start_time": "2017-01-24T19:53:00.343091"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "\n",
    "#model = GradientBoostingClassifier()\n",
    "pca = PCA()#n_components=\"mle\",svd_solver=\"full\")\n",
    "norm = Normalizer()\n",
    "#model=GradientBoostingClassifier()\n",
    "model = RandomForestClassifier()\n",
    "#model = SVC(probability=True)\n",
    "pipeline= Pipeline([('normalize', norm),('reduce_dim', pca),(\"model\",model)])\n",
    "try:\n",
    "    # use best parameters if available\n",
    "    #\n",
    "    #pipeline.set_params(**grid_result.best_params_)\n",
    "    print \"using last optimized model\"\n",
    "except:\n",
    "    print \"no optim result, or bad ones: let's keep the default ones\"\n",
    "    pass\n",
    "scores = cross_val_score(pipeline, feature_mat, label_mat,cv=3,verbose=1,scoring=\"accuracy\",n_jobs=6)                          \n",
    "print \"accurracy: %g, std(%g))\" % (scores.mean(), scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-24T19:54:43.067653",
     "start_time": "2017-01-24T19:54:43.059970"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-24T19:54:52.103615",
     "start_time": "2017-01-24T19:54:47.207602"
    },
    "collapsed": false,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, accuracy_score,precision_recall_curve, auc\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(feature_mat,label_mat,test_size=0.3)\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "\n",
    "fitted = pipeline.fit(X_train,Y_train)\n",
    "probs = fitted.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "preds_train = fitted.predict_proba(X_train)[:,1]\n",
    "fpr, tpr, threshold = roc_curve(Y_test, preds)\n",
    "fpr_train, tpr_train, threshold_train = roc_curve(Y_train, preds_train)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "roc_auc_train = auc(fpr_train, tpr_train)\n",
    "precision, recall, ths = precision_recall_curve(Y_test, preds)\n",
    "precision_train, recall_train, ths_train = precision_recall_curve(Y_train, preds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-24T19:54:52.204783",
     "start_time": "2017-01-24T19:54:52.104850"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bokeh.models.ranges import Range1d\n",
    "#print \"auc: %.2g, on train: %.2g\" %(roc_auc, roc_auc_train)\n",
    "roc_df = pd.DataFrame({\"fpr\":fpr,\"tpr\":tpr}).set_index(\"fpr\")\n",
    "pr_df = pd.DataFrame({\"precision\": precision, \"recall\":recall}).set_index(\"recall\")\n",
    "roc_df[\"diag\"] = roc_df.index\n",
    "pr_df[\"random\"] = pr_df.precision.iloc[0]\n",
    "\n",
    "# roc curve\n",
    "roc_f = figure(width=400,height=400,title=\"roc, auc: %.2g, on train: %.2g\"  %(roc_auc, roc_auc_train) )\n",
    "roc_f.xaxis.axis_label = \"tpr\"\n",
    "auc_range= Range1d(0,1)\n",
    "roc_f.x_range = auc_range \n",
    "roc_f.y_range = auc_range \n",
    "roc_f.yaxis.axis_label = \"fpr\"\n",
    "roc_f.cross(fpr,tpr,size=5)\n",
    "roc_f.line(fpr,tpr,legend=\"roc\")\n",
    "roc_f.circle(fpr_train,tpr_train,size=5,color=\"red\", line_width=1)\n",
    "roc_f.line(fpr_train,tpr_train,color=\"red\",legend=\"roc on train\")\n",
    "roc_f.line([0,1],[0,1], color=\"grey\")\n",
    "\n",
    "# pr curve\n",
    "pr_f = figure(width=400,height=400,title=\"PR curve\")\n",
    "pr_f.xaxis.axis_label = \"recall\"\n",
    "pr_f.yaxis.axis_label = \"precision\"\n",
    "pr_f.cross(recall,precision,size=5)\n",
    "pr_f.line(recall,precision,legend=\"PR\")\n",
    "pr_f.circle(recall_train,precision_train,size=5,color=\"red\", line_width=1)\n",
    "pr_f.line(recall_train,precision_train,color=\"red\",legend=\"PR on train\")\n",
    "\n",
    "show(row(\n",
    "    pr_f,\n",
    "    roc_f\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-24T19:58:40.991191",
     "start_time": "2017-01-24T19:58:40.977695"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame({\"importance\":model.feature_importances_}).set_index(feature_set.columns)\n",
    "feature_imp.sort_values(by=\"importance\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimisation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model : GradientBoostingClassifier, parameters: \n",
    "#loss : {‘deviance’, ‘exponential’},\n",
    "#learning_rate : float, optional (default=0.1)\n",
    "#n_estimators : int (default=100)\n",
    "#max_depth : integer, optional (default=3)\n",
    "#min_samples_split : int, float, optional (default=2)\n",
    "grids=dict()\n",
    "XDB_param_grid = {\n",
    "    #\"model__loss\":  [\"deviance\", 'exponential'],\n",
    "    \"model__learning_rate\" : [1e-3,0.01, 0.1],\n",
    "    \"model__n_estimators\" : [10, 50, 100, 150],\n",
    "    \"model__max_depth\" : [5,10,15],\n",
    "    \"model__min_samples_split\" : [5,10,20]\n",
    "}\n",
    "grids[GradientBoostingClassifier] = XDB_param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model : RandomForestClassifier, parameters: \n",
    "# n_estimators : int (default=100)\n",
    "# criterion : \"gini\",\"entropy\"\n",
    "# max_features : auto , fraction\n",
    "# max_depth : integer, optional (default=3)\n",
    "# min_samples_split : int, float, optional (default=2)\n",
    "\n",
    "RF_param_grid = {\n",
    "    #\"model__criterion\":  [\"gini\", \"entropy\"],\n",
    "    \"model__n_estimators\" : [75,100,150,200],\n",
    "    #\"model__max_features\" : [\"auto\",0.5,0.25,0.1],\n",
    "    \"model__max_depth\" : [2,5,10,20],\n",
    "    \"model__min_samples_split\" : [5,10,20]\n",
    "}\n",
    "grids[RandomForestClassifier] = RF_param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# C :penalty\n",
    "# kernel : ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ \n",
    "SVC_param_grid = {\n",
    "   'model__C': [1e-7,1e-6,1e-5,0.1],\n",
    "    \"model__kernel\": [\"rbf\",\"linear\"],\n",
    "    #\"model_degree\" : [1,3,5], # polynomial degrees\n",
    "    \"model__gamma\" : [\"auto\"], # kernel coef (rbf)\n",
    "    #\"coef0\" # for poly, signmoid\n",
    "    \"model__tol\" : [1e-7,1e-6,1e-5, 1e-4,1e-3]\n",
    "}\n",
    "grids[SVC] = SVC_param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m  = type(dict(pipeline.steps)[\"model\"])\n",
    "param_grid=grids[m]\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=6, shuffle=True)\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring=\"accuracy\", n_jobs=-1, verbose=1,cv=kfold)\n",
    "grid_result = grid_search.fit(feature_mat,label_mat)\n",
    "\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "#for mean, stdev, param in zip(means, stds, params):\n",
    "#    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "pipeline_optimizer = TPOTClassifier(\n",
    "    generations=20, # the more generatiion, the more optimized you get\n",
    "    population_size=100,\n",
    "    num_cv_folds=4,\n",
    "    scoring=\"accuracy\",\n",
    "    random_state=42,\n",
    "    verbosity=3)\n",
    "\n",
    "pipeline_optimizer.fit(feature_mat, label_mat)\n",
    "print pipeline_optimizer.score(feature_mat, label_mat)\n",
    "pipeline_optimizer.export('tpot_longrun_exported_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline_optimizer.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
